# -*- coding: utf-8 -*-
"""text_generation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VpIvRLHpS7VXSSXEsp_91hi28_JTZOX4

Нужно создать и обучить генеративную нейронную сеть с использованием RNN-слоёв и механизма внимания

После обучения модели написать функцию, которая
принимает на вход текст и возвращает сгенерированное продолжение.

## Описание датасета

Ссылка на датасет: https://huggingface.co/datasets/ScoutieAutoML/russian-news-telegram-dataset

Датасет собранный из 30 русскоязычных новостных Telegram каналов на тему Новости и СМИ, собранный и размеченный автоматически с помощью сервиса сбора и разметки данных Скаути.

Колонки датасета:
- taskId - идентификатор задачи в сервисе Скаути.
- text - основной текст.
- url - ссылка на публикацию.
- sourceLink - ссылка на Telegram.
- subSourceLink - ссылка на канал.
- views - просмотры текста.
- likes - для данного датасета пустое поле (означающее количество эмоций).
- createTime - дата публикации в формате unix time.
- createTime - дата сбора публикации в формате unix time.
- clusterId - id кластера.
- vector - embedding текста (его векторное представление).
- ners - массив выявленных именованных сущностей, где lemma - лемматизированное представление слова, а label это название тега, start_pos - начальная позиция сущности в тексте, end_pos - конечная позиция сущности в тексте.
- sentiment - эмоциональный окрас текста: POSITIVE, NEGATIVE, NEUTRAL.
- language - язык текста RUS, ENG.
- spam - классификация текста, как рекламный или нет NOT_SPAM - нет рекламы, иначе SPAM - текст помечен, как рекламный.
- length - количество токенов в тексте (слов).
- markedUp - означает, что текст размечен или нет в рамках сервиса Скаути принимает значение true или false.

## Imports
"""

!pip install datasets pandas

import pandas as pd
from datasets import load_dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import ast
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Attention
from tensorflow.keras.optimizers import Adam
from nltk.translate.bleu_score import sentence_bleu
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')

"""## EDA"""

dataset = load_dataset(
    "ScoutieAutoML/russian-news-telegram-dataset",
    name="default"
)


df_train = dataset["train"].to_pandas()

df_train = df_train.sample(n=8000, random_state=42)

"""*Ограничили первыми 8.000 строками, потому что ОЗУ не вывозит((*"""

df_train.head(5)

"""*Удалим столбцы, которые нам точно не пригодятся для анализа датасета*"""

columns_to_drop = [
    "id", "title", "task_id", "url", "source_link", "subsource_link",
    "views", "likes", "create_time", "cluster_id", "ners", "source", "subsource", "marked_up", "crawl_time", "vector", "doc_length"
]
df_cleaned = df_train.drop(columns=columns_to_drop)

df_cleaned.head(5)

"""*Как мы видим, столбец statistics содержит в себе много полезной для нас информации. Рассмотрим более детально*"""

stats_df = df_train["statistics"].apply(pd.Series)

stats_df

"""*Ну ладно, не много информации, только длина текста, которую мы сейчас добавим в общий датасет.*"""

df_cleaned["length"] = df_cleaned["statistics"].apply(lambda x: x["length"])
df_cleaned = df_cleaned.drop(columns=["statistics"])

df_cleaned.head()

"""*Суперрр! Радуемся мелочам, потому что в конце радоваться будет нечему...*"""

plt.figure(figsize=(8, 5))
sns.countplot(data=df_cleaned, x="language", palette="magma")
plt.title("Распределение языков")
plt.xlabel("Язык")
plt.ylabel("Количество текстов")
plt.show()

"""*Уберем other и eng*"""

df_cleaned = df_cleaned[df_cleaned["language"] == "rus"]

df_cleaned["language"].value_counts()

plt.figure(figsize=(8, 5))
sns.countplot(data=df_cleaned, x="emo", palette="viridis")
plt.title("Распределение эмоционального окраса")
plt.xlabel("Эмоциональный окрас")
plt.ylabel("Количество текстов")
plt.show()

"""*То, что они не сбалансированны нас пугать не должно. Потому что мы просто анализируем. Исходя из графика, видно, что русскоязычные телеграм каналы содержат больше нейтральных новостей и сми, а не позитивных или негативных, что странно, на самом деле. Интерестно (или нет)*"""

plt.figure(figsize=(8, 5))
sns.countplot(data=df_cleaned, x="spam", palette="plasma")
plt.title("Распределение спама")
plt.xlabel("Спам")
plt.ylabel("Количество текстов")
plt.show()

"""*Вау...меня поражают новостные телеграм каналы, которые нейтральные по стилистике, да еще и без спама. Ну окэй*"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df_cleaned, x="length", bins=50, kde=True, color="blue")
plt.title("Распределение длины текстов")
plt.xlabel("Длина текста (количество токенов)")
plt.ylabel("Количество текстов")
plt.show()

"""*Удалим слишком длинные и слишком короткие тексты*"""

df_cleaned = df_cleaned[df_cleaned["length"] >= 10]
df_cleaned = df_cleaned[df_cleaned["length"] <= 300]

plt.figure(figsize=(10, 6))
sns.histplot(df_cleaned["length"], bins=50, kde=True, color="blue")
plt.title("Распределение длины текстов после обработки")
plt.xlabel("Длина текста (количество токенов)")
plt.ylabel("Количество текстов")
plt.show()

df_cleaned.isnull().any()

df_cleaned.duplicated().any()

df_cleaned.drop_duplicates(inplace=True)

df_cleaned.duplicated().any()

df_cleaned.info()

"""## Preprocessing"""

def clean_text(text):
    text = re.sub(r"http\\S+|www\\S+|https\\S+", "", text, flags=re.MULTILINE)
    text = re.sub(r"\\@\\w+|\\#", "", text)
    text = re.sub(r"[^\w\s.,!?]", "", text)
    text = text.lower()
    text = text.strip()
    text = re.sub(r"[\\n\\\\]", " ", text)
    return text

df_cleaned["text"] = df_cleaned["text"].apply(clean_text)

df_cleaned["text"]

"""## RNN"""

tokenizer = Tokenizer(num_words=None)
tokenizer.fit_on_texts(df_cleaned["text"])
word_index = tokenizer.word_index
print(f"Размер словаря: {len(word_index)}")

sequences = tokenizer.texts_to_sequences(df_cleaned["text"])

max_sequence_length = 80
data = pad_sequences(sequences, maxlen=max_sequence_length, padding="post", truncating="post")

n_samples = 8000
X = data[:, :-1]
y = data[:, -1]
X = X[:n_samples]
y = y[:n_samples]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

!free -h

inputs = Input(shape=(X_train.shape[1],))

vocab_size = len(word_index) + 1
embedding_dim = 300
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)

lstm_out = LSTM(512, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(x)
attention = Attention()([lstm_out, lstm_out])
context = Dense(512, activation='tanh')(attention)

outputs = Dense(vocab_size, activation='softmax')(context[:, -1, :])

model = Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

def create_dataset(X, y, batch_size=16):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    dataset = dataset.shuffle(buffer_size=len(X)).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset


batch_size = 16
train_dataset = create_dataset(X_train, y_train)
test_dataset = create_dataset(X_test, y_test)

callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.00001)
]

history = model.fit(train_dataset,
                    validation_data=test_dataset,
                    epochs=50,
                    verbose=1,
                    callbacks=callbacks)

"""*Ну слава богу, на тысячную попытку, где accuracy было не больше 0.4, я считаю, что этот результат хорош*"""

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""*Модель демонстрирует хорошую сходимость на тренировочных данных, но признаки переобучения появляются после 4-й эпохи: val_loss растет, а val_accuracy стабилизируется.*"""

def generate_text(seed_text, max_length=50, temperature=1.0):
    seed_tokens = tokenizer.texts_to_sequences([seed_text])[0]
    seed_padded = pad_sequences([seed_tokens], maxlen=X_train.shape[1], padding='post', truncating='post')

    generated = seed_tokens.copy()
    for _ in range(max_length):
        pred = model.predict(seed_padded, verbose=0)[0]
        pred = pred + np.random.uniform(0, 0.05, len(pred))
        pred = np.log(pred + 1e-10) / temperature
        pred = np.exp(pred) / np.sum(np.exp(pred))
        next_token = np.random.choice(len(pred), p=pred)
        if next_token == 0 or len(generated) >= max_sequence_length:
            break
        generated.append(next_token)
        seed_padded = pad_sequences([generated[-X_train.shape[1]:]], maxlen=X_train.shape[1], padding='post', truncating='post')

    return ' '.join(tokenizer.index_word.get(token, '') for token in generated)

# Тестовая генерация
print("Пример генерации текста:")
print(generate_text("в москве", temperature=1.0))
print(generate_text("сша объявили", temperature=1.0))

"""*Как нейролингвист могу сказать - у этой генерации есть признаки сенсорной афазии. Есть даже для этого термин специальный - речевой салат.*

*Раньше у нее получалось воспроизводить только "в москве на" и все. (А это уже была моторная афазия)*

*Извините, у меня профдеформация(*

*Уменьшим temperature до 0.6, чтобы уменьшить вероятность случайных токенов*
"""

def generate_text(seed_text, max_length=10, temperature=0.6):
    seed_tokens = tokenizer.texts_to_sequences([seed_text])[0]
    seed_padded = pad_sequences([seed_tokens], maxlen=X_train.shape[1], padding='post', truncating='post')

    generated = seed_tokens.copy()
    for _ in range(max_length):
        pred = model.predict(seed_padded, verbose=0)[0]
        pred = pred + np.random.uniform(0, 0.05, len(pred))
        pred = np.log(pred + 1e-10) / temperature
        pred = np.exp(pred) / np.sum(np.exp(pred))
        next_token = np.random.choice(len(pred), p=pred)
        if next_token == 0 or len(generated) >= max_sequence_length:
            break
        generated.append(next_token)
        seed_padded = pad_sequences([generated[-X_train.shape[1]:]], maxlen=X_train.shape[1], padding='post', truncating='post')

    return ' '.join(tokenizer.index_word.get(token, '') for token in generated)

print("Пример генерации текста:")
print(generate_text("в москве", temperature=0.6))
print(generate_text("сша объявили", temperature=0.6))

def generate_text(seed_text, max_length=10, temperature=0.6):
    seed_tokens = tokenizer.texts_to_sequences([seed_text])[0]
    seed_padded = pad_sequences([seed_tokens], maxlen=X_train.shape[1], padding='post', truncating='post')

    generated = seed_tokens.copy()
    for _ in range(max_length):
        pred = model.predict(seed_padded, verbose=0)[0]
        pred = pred + np.random.uniform(0, 0.02, len(pred))
        pred = np.log(pred + 1e-10) / temperature
        pred = np.exp(pred) / np.sum(np.exp(pred))
        next_token = np.random.choice(len(pred), p=pred)
        if next_token == 0 or len(generated) >= max_sequence_length:
            break
        generated.append(next_token)
        seed_padded = pad_sequences([generated[-X_train.shape[1]:]], maxlen=X_train.shape[1], padding='post', truncating='post')

    return ' '.join(tokenizer.index_word.get(token, '') for token in generated)

temperatures = [1.0, 0.8, 0.6, 0.5, 0.3]
print("Примеры генерации текста с разными значениями temperature:")
for temp in temperatures:
    print(f"\nTemperature = {temp}:")
    print(generate_text("в москве", temperature=temp))
    print(generate_text("сша объявили", temperature=temp))

"""*Вывод: С уменьшением temperature тексты становятся более осмысленными, но теряют разнообразие. Оптимально — temperature=0.5–0.6 для баланса "связности и разнообразия".*"""

nltk.download('punkt_tab')

from nltk.translate.bleu_score import sentence_bleu
from nltk.tokenize import word_tokenize

reference = word_tokenize("в москве прошел парад победы")
hypothesis = word_tokenize(generate_text("в москве", max_length=10, temperature=0.6).lower())
bleu_score = sentence_bleu([reference], hypothesis)
print(f"BLEU-Score для сгенерированного текста (temperature=0.6): {bleu_score}")


generated_text = generate_text("в москве", max_length=10, temperature=0.6)
print(f"Сгенерированный текст: {generated_text}")

"""*Ну вобщем, да, цифры не радуют, но, как говорится, есть к чему стремиться в дальнейшем)*"""